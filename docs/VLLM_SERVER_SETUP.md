# üöÄ vLLM Server Setup - T560 (L4 24GB)

## Vis√£o Geral

Este documento descreve a configura√ß√£o completa do servidor de infer√™ncia vLLM no T560 para servir o modelo **Qwen 2.5 32B GPTQ** (quantizado para caber em 18GB).

## Hardware

- **GPU**: NVIDIA L4 24GB
- **CUDA**: 12.4+
- **Driver**: 545+
- **Espa√ßo em disco**: ~50GB livre (para modelo + ambiente)

## Arquitetura

```
T560
‚îú‚îÄ‚îÄ vLLM Server (porta 8001)
‚îÇ   ‚îú‚îÄ‚îÄ Qwen 2.5 32B GPTQ (~18GB)
‚îÇ   ‚îú‚îÄ‚îÄ GPU Memory: 90% utilization
‚îÇ   ‚îî‚îÄ‚îÄ Max context: 8192 tokens
‚îî‚îÄ‚îÄ OpenAI-compatible API
    ‚îú‚îÄ‚îÄ /v1/models
    ‚îú‚îÄ‚îÄ /v1/completions
    ‚îî‚îÄ‚îÄ /v1/chat/completions
```

## Setup R√°pido

### 1. Executar Script de Setup (LOCALMENTE na T560)

```bash
# Na m√°quina T560 (local)
cd ~/beagle/scripts/infrastructure
chmod +x setup_vllm_t560.sh
./setup_vllm_t560.sh
```

**Nota**: Execute diretamente na m√°quina T560, n√£o via SSH.

O script faz automaticamente:
- ‚úÖ Atualiza sistema
- ‚úÖ Instala CUDA 12.4 (se necess√°rio)
- ‚úÖ Instala Python 3.11
- ‚úÖ Cria virtual environment
- ‚úÖ Instala vLLM + PyTorch
- ‚úÖ Configura HuggingFace
- ‚úÖ Baixa modelo Qwen 2.5 32B GPTQ
- ‚úÖ Cria script de start

**Tempo estimado**: 30-60 minutos (principalmente download do modelo)

### 2. Iniciar Servidor

```bash
# Em tmux (para manter rodando)
tmux new -s vllm
~/start_vllm.sh

# Detach: Ctrl+B, D
# Reattach: tmux attach -t vllm
```

### 3. Testar

```bash
# Teste b√°sico
curl http://localhost:8001/v1/models

# Teste completo
~/beagle/scripts/infrastructure/test_vllm.sh
```

## Uso Detalhado

### Vari√°veis de Ambiente

```bash
# Porta do servidor (padr√£o: 8001)
export VLLM_PORT=8001

# Host (padr√£o: 0.0.0.0 = todas interfaces)
export VLLM_HOST=0.0.0.0

# Diret√≥rio do modelo
export MODEL_DIR=~/models/qwen-32b-gptq
```

### Iniciar com Op√ß√µes Customizadas

```bash
~/start_vllm.sh \
  --max-model-len 16384 \
  --gpu-memory-utilization 0.95 \
  --tensor-parallel-size 1
```

### Acessar de Outro Host (Opcional)

Se quiser acessar o servidor de outra m√°quina na rede:

1. **Obter IP do T560**:
   ```bash
   ip addr show | grep "inet "
   # Exemplo: 192.168.1.100
   ```

2. **Configurar firewall** (se necess√°rio):
   ```bash
   sudo ufw allow 8001/tcp
   ```

3. **Usar no c√≥digo**:
   ```rust
   // Em beagle-llm
   let client = AnthropicClient::new(
       "http://192.168.1.100:8001/v1/completions".to_string(),
       api_key,
   );
   ```

**Nota**: Para uso local, use `http://localhost:8001` ou `http://127.0.0.1:8001`

## API Endpoints

### Listar Modelos

```bash
curl http://localhost:8001/v1/models
```

**Resposta**:
```json
{
  "object": "list",
  "data": [
    {
      "id": "qwen-32b-gptq",
      "object": "model",
      "created": 1234567890,
      "owned_by": "vllm"
    }
  ]
}
```

### Completions

```bash
curl -X POST http://localhost:8001/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-32b-gptq",
    "prompt": "Explain pharmacokinetics:",
    "max_tokens": 200,
    "temperature": 0.7
  }'
```

### Chat Completions

```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-32b-gptq",
    "messages": [
      {"role": "user", "content": "What is PBPK modeling?"}
    ],
    "max_tokens": 200,
    "temperature": 0.7
  }'
```

## Monitoramento

### GPU Usage

```bash
# Em tempo real
watch -n 1 nvidia-smi

# Ou
nvidia-smi -l 1
```

### Logs do Servidor

```bash
# Se rodando em tmux
tmux attach -t vllm

# Ou verificar logs do sistema
journalctl -u vllm -f  # Se configurado como servi√ßo
```

### Performance

- **Throughput**: ~10-20 tokens/s (depende do prompt)
- **Lat√™ncia**: ~200-500ms primeiro token
- **GPU Memory**: ~18-20GB usado
- **Context Window**: 8192 tokens (configur√°vel)

## Troubleshooting

### Servidor n√£o inicia

1. **Verificar GPU**:
   ```bash
   nvidia-smi
   ```

2. **Verificar modelo**:
   ```bash
   ls -lh ~/models/qwen-32b-gptq
   ```

3. **Verificar virtual environment**:
   ```bash
   source ~/vllm-env/bin/activate
   python -c "import vllm; print(vllm.__version__)"
   ```

### Out of Memory

- Reduzir `--gpu-memory-utilization` (padr√£o: 0.90)
- Reduzir `--max-model-len` (padr√£o: 8192)
- Usar modelo menor ou quantiza√ß√£o mais agressiva

### Conex√£o Recusada

1. **Verificar firewall**:
   ```bash
   sudo ufw status
   sudo ufw allow 8001/tcp
   ```

2. **Verificar se servidor est√° rodando**:
   ```bash
   netstat -tlnp | grep 8001
   ```

3. **Verificar host binding**:
   - Se `VLLM_HOST=127.0.0.1`, s√≥ aceita conex√µes locais
   - Use `VLLM_HOST=0.0.0.0` para aceitar de qualquer IP

### Modelo n√£o encontrado

```bash
# Re-baixar modelo
huggingface-cli login
huggingface-cli download Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4 \
  --local-dir ~/models/qwen-32b-gptq
```

## Integra√ß√£o com Beagle

### Configurar beagle-llm

```rust
// Em crates/beagle-llm/src/client.rs
pub struct AnthropicClient {
    base_url: String,  // http://192.168.1.100:8001/v1
    api_key: String,
}

// Usar endpoint OpenAI-compatible
let client = AnthropicClient::new(
    "http://192.168.1.100:8001/v1".to_string(),
    "dummy".to_string(),  // vLLM n√£o precisa de key real
);
```

### Roteamento

Adicionar vLLM como op√ß√£o no roteamento de modelos:

```rust
match model_type {
    ModelType::Qwen32B => {
        // Usar vLLM server
        self.vllm_client.complete(request).await
    }
    _ => {
        // Usar Claude/Gemini
        self.anthropic_client.complete(request).await
    }
}
```

## Manuten√ß√£o

### Atualizar vLLM

```bash
source ~/vllm-env/bin/activate
pip install --upgrade vllm
```

### Atualizar Modelo

```bash
# Baixar nova vers√£o
huggingface-cli download Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4 \
  --local-dir ~/models/qwen-32b-gptq-new

# Testar
VLLM_MODEL_DIR=~/models/qwen-32b-gptq-new ~/start_vllm.sh

# Se OK, substituir
mv ~/models/qwen-32b-gptq ~/models/qwen-32b-gptq-old
mv ~/models/qwen-32b-gptq-new ~/models/qwen-32b-gptq
```

### Backup

```bash
# Backup do modelo (18GB)
tar -czf qwen-32b-gptq-backup.tar.gz ~/models/qwen-32b-gptq

# Backup do virtual environment (opcional)
tar -czf vllm-env-backup.tar.gz ~/vllm-env
```

## Refer√™ncias

- [vLLM Documentation](https://docs.vllm.ai/)
- [Qwen 2.5 Models](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4)
- [OpenAI API Compatibility](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)

---

**√öltima atualiza√ß√£o**: 2025-01-XX
**Status**: ‚úÖ Configurado e testado

