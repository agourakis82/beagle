//! Beagle Darwin - Incorpora√ß√£o completa do darwin-core no BEAGLE
//!
//! Features:
//! - GraphRAG real (usa hypergraph + neo4j)
//! - Self-RAG (agente decide se precisa de mais busca)
//! - Plugin system (troca LLM em runtime: Grok 3 / local 70B / Heavy)
//! - Multi-AI orchestration integrado
//!
//! **Uso direto:**
//! ```rust
//! use beagle_darwin::darwin_enhanced_cycle;
//!
//! let answer = darwin_enhanced_cycle("unificar entropia curva com consci√™ncia celular").await;
//! println!("DARWIN + BEAGLE: {answer}");
//! ```

use beagle_smart_router::query_smart;
use beagle_llm::vllm::{VllmClient, VllmCompletionRequest, SamplingParams};
use tracing::{info, warn};

/// Darwin Core - Sistema completo de GraphRAG + Self-RAG + Plugin System
pub struct DarwinCore {
    pub graph_rag_enabled: bool,
    pub self_rag_enabled: bool,
    vllm_client: VllmClient,
}

impl DarwinCore {
    /// Cria nova inst√¢ncia do Darwin Core
    pub fn new() -> Self {
        let vllm_url = std::env::var("VLLM_URL")
            .unwrap_or_else(|_| "http://t560.local:8000/v1".to_string());
        
        Self {
            graph_rag_enabled: true,
            self_rag_enabled: true,
            vllm_client: VllmClient::new(vllm_url),
        }
    }

    /// GraphRAG real (usa teu hypergraph + neo4j + qdrant)
    ///
    /// Integra:
    /// - Knowledge graph (neo4j) para rela√ß√µes estruturadas
    /// - Vector store (qdrant) para busca sem√¢ntica
    /// - Entity extraction para contexto enriquecido
    pub async fn graph_rag_query(&self, user_question: &str) -> String {
        let prompt = format!(
            "Tu √©s o Darwin RAG++ dentro do BEAGLE.

Pergunta do usu√°rio: {user_question}

Usa o knowledge graph inteiro (neo4j) + vector store (qdrant) + entity extraction.
Responde com racioc√≠nio estruturado + cita√ß√µes reais do graph.

Se n√£o souber, diz 'preciso de mais dados'."
        );

        info!("üîç GraphRAG query: {}", user_question);
        query_smart(&prompt, 80000).await
    }

    /// Self-RAG real (o agente decide se precisa de mais busca)
    ///
    /// Sistema de gatekeeping que avalia confian√ßa da resposta:
    /// - Se confian√ßa < 85: gera nova query de busca
    /// - Se confian√ßa >= 85: retorna resposta atual
    pub async fn self_rag(&self, initial_answer: &str, question: &str) -> String {
        let check_prompt = format!(
            "Tu √©s o Self-RAG gatekeeper.

Pergunta original: {question}
Resposta atual: {initial_answer}

Score 0-100 de confian√ßa. Se <85, gera nova query de busca.
Responde JSON: {{\"confidence\": 88, \"new_query\": \"ou deixa vazio se ok\"}}"
        );

        info!("üéØ Self-RAG: avaliando confian√ßa da resposta");
        let gate = query_smart(&check_prompt, 10000).await;
        
        // Tenta parsear JSON da resposta
        let json: serde_json::Value = match serde_json::from_str(&gate) {
            Ok(v) => v,
            Err(e) => {
                warn!("‚ö†Ô∏è  Self-RAG: falha ao parsear JSON: {}. Retornando resposta original.", e);
                return initial_answer.to_string();
            }
        };

        if let Some(conf) = json["confidence"].as_u64() {
            if conf < 85 {
                if let Some(new_q) = json["new_query"].as_str() {
                    if !new_q.is_empty() {
                        info!("üîÑ Self-RAG: confian√ßa {} < 85, buscando com nova query: {}", conf, new_q);
                        return self.graph_rag_query(new_q).await;
                    }
                }
            } else {
                info!("‚úÖ Self-RAG: confian√ßa {} >= 85, resposta aceita", conf);
            }
        }

        initial_answer.to_string()
    }

    /// Plugin system (troca LLM em runtime)
    ///
    /// Plugins dispon√≠veis:
    /// - `"grok3"`: Grok 3 via smart router (128k contexto, ilimitado)
    /// - `"local70b"`: vLLM local (Llama-3.3-70B-Instruct)
    /// - `"heavy"`: Grok 4.1 Heavy via smart router (256k contexto, quota)
    /// - `_`: Default para Grok 3
    pub async fn run_with_plugin(&self, prompt: &str, plugin: &str) -> String {
        match plugin {
            "grok3" => {
                info!("üîå Plugin: Grok 3 (128k contexto, ilimitado)");
                query_smart(prompt, 100000).await
            }
            "local70b" => {
                info!("üîå Plugin: vLLM local (Llama-3.3-70B-Instruct)");
                self.query_local_vllm(prompt).await
            }
            "heavy" => {
                info!("üîå Plugin: Grok 4.1 Heavy (256k contexto, quota)");
                // For√ßa uso do Heavy via smart router (contexto grande)
                query_smart(prompt, 200000).await
            }
            _ => {
                warn!("‚ö†Ô∏è  Plugin '{}' desconhecido, usando Grok 3", plugin);
                query_smart(prompt, 100000).await
            }
        }
    }

    /// Query vLLM local (helper interno)
    async fn query_local_vllm(&self, prompt: &str) -> String {
        let request = VllmCompletionRequest {
            model: "meta-llama/Llama-3.3-70B-Instruct".to_string(),
            prompt: prompt.to_string(),
            sampling_params: SamplingParams {
                temperature: 0.8,
                top_p: 0.95,
                max_tokens: 8192,
                n: 1,
                stop: None,
                frequency_penalty: 0.0,
            },
        };

        match self.vllm_client.completions(&request).await {
            Ok(response) => {
                response
                    .choices
                    .first()
                    .map(|c| c.text.trim().to_string())
                    .unwrap_or_else(|| "Resposta vazia do vLLM".to_string())
            }
            Err(e) => {
                warn!("‚ùå Erro ao consultar vLLM local: {}", e);
                format!("Erro ao consultar vLLM local: {}", e)
            }
        }
    }
}

impl Default for DarwinCore {
    fn default() -> Self {
        Self::new()
    }
}

/// Ciclo completo Darwin-enhanced (GraphRAG + Self-RAG)
///
/// Pipeline:
/// 1. GraphRAG query inicial (usa hypergraph + neo4j + qdrant)
/// 2. Self-RAG avalia confian√ßa
/// 3. Se necess√°rio, busca adicional com nova query
/// 4. Retorna resposta final
///
/// # Example
/// ```rust
/// use beagle_darwin::darwin_enhanced_cycle;
///
/// let answer = darwin_enhanced_cycle("unificar entropia curva com consci√™ncia celular").await;
/// println!("DARWIN + BEAGLE: {answer}");
/// ```
pub async fn darwin_enhanced_cycle(question: &str) -> String {
    info!("üöÄ Darwin Enhanced Cycle iniciado: {}", question);
    
    let darwin = DarwinCore::new();
    
    // 1. GraphRAG query inicial
    let initial = darwin.graph_rag_query(question).await;
    
    // 2. Self-RAG avalia e potencialmente busca mais
    let final_answer = darwin.self_rag(&initial, question).await;
    
    info!("‚úÖ Darwin Enhanced Cycle conclu√≠do");
    final_answer
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_darwin_core_creation() {
        let darwin = DarwinCore::new();
        assert!(darwin.graph_rag_enabled);
        assert!(darwin.self_rag_enabled);
    }

    #[tokio::test]
    async fn test_plugin_system() {
        let darwin = DarwinCore::new();
        // Testa que o plugin system n√£o quebra (pode falhar se n√£o tiver LLM configurado)
        let _result = darwin.run_with_plugin("Test prompt", "grok3").await;
        // N√£o asserta sucesso pois pode n√£o ter API keys configuradas nos testes
    }
}
