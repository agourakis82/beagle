//! Beagle Darwin - Incorpora√ß√£o completa do darwin-core no BEAGLE
//!
//! Features:
//! - GraphRAG real (usa hypergraph + neo4j)
//! - Self-RAG (agente decide se precisa de mais busca)
//! - Plugin system (troca LLM em runtime: Grok 3 / local 70B / Heavy)
//! - Multi-AI orchestration integrado
//!
//! **Uso direto:**
//! ```rust
//! use beagle_darwin::darwin_enhanced_cycle;
//!
//! let answer = darwin_enhanced_cycle("unificar entropia curva com consci√™ncia celular").await;
//! println!("DARWIN + BEAGLE: {answer}");
//! ```

use beagle_core::BeagleContext;
use beagle_llm::vllm::{SamplingParams, VllmClient, VllmCompletionRequest};
use beagle_smart_router::query_smart;
use std::sync::Arc;
use tracing::{info, warn};

/// Darwin Core - Sistema completo de GraphRAG + Self-RAG + Plugin System
pub struct DarwinCore {
    pub graph_rag_enabled: bool,
    pub self_rag_enabled: bool,
    ctx: Option<Arc<BeagleContext>>,
    vllm_client: VllmClient,
}

impl DarwinCore {
    /// Cria nova inst√¢ncia do Darwin Core (modo legacy, sem BeagleContext)
    pub fn new() -> Self {
        let vllm_url =
            std::env::var("VLLM_URL").unwrap_or_else(|_| "http://t560.local:8000/v1".to_string());

        Self {
            graph_rag_enabled: true,
            self_rag_enabled: true,
            ctx: None,
            vllm_client: VllmClient::new(vllm_url),
        }
    }

    /// Cria nova inst√¢ncia do Darwin Core com BeagleContext
    pub fn with_context(ctx: Arc<BeagleContext>) -> Self {
        let vllm_url = ctx
            .cfg
            .llm
            .vllm_url
            .clone()
            .unwrap_or_else(|| "http://t560.local:8000/v1".to_string());

        Self {
            graph_rag_enabled: true,
            self_rag_enabled: true,
            ctx: Some(ctx),
            vllm_client: VllmClient::new(vllm_url),
        }
    }

    /// GraphRAG real (usa teu hypergraph + neo4j + qdrant)
    ///
    /// Integra:
    /// - Knowledge graph (neo4j) para rela√ß√µes estruturadas
    /// - Vector store (qdrant) para busca sem√¢ntica
    /// - Entity extraction para contexto enriquecido
    pub async fn graph_rag_query(&self, user_question: &str) -> String {
        // Se temos BeagleContext, usa as traits
        if let Some(ctx) = &self.ctx {
            info!("üîç GraphRAG query (com BeagleContext): {}", user_question);
            
            // 1. Busca no vector store
            let vectors = match ctx.vector.query(user_question, 10).await {
                Ok(v) => v,
                Err(e) => {
                    warn!("Erro ao buscar no vector store: {}", e);
                    vec![]
                }
            };

            // 2. Busca no graph store
            let graph_result = match ctx.graph.cypher_query(
                "MATCH (n)-[r]->(m) WHERE n.name CONTAINS $query OR m.name CONTAINS $query RETURN n, r, m LIMIT 20",
                serde_json::json!({"query": user_question}),
            ).await {
                Ok(g) => g,
                Err(e) => {
                    warn!("Erro ao buscar no graph store: {}", e);
                    serde_json::json!({})
                }
            };

            // 3. Monta contexto enriquecido
            let context = format!(
                "Vector results: {}\nGraph results: {}",
                vectors.len(),
                graph_result.get("results").and_then(|r| r.as_array()).map(|a| a.len()).unwrap_or(0)
            );

            let prompt = format!(
                "Tu √©s o Darwin RAG++ dentro do BEAGLE.

Pergunta do usu√°rio: {user_question}

Contexto do knowledge graph:
{context}

Responde com racioc√≠nio estruturado + cita√ß√µes reais do graph.

Se n√£o souber, diz 'preciso de mais dados'."
            );

            // 4. Usa LLM do contexto
            match ctx.llm.complete(&prompt).await {
                Ok(answer) => answer,
                Err(e) => {
                    warn!("Erro ao consultar LLM: {}, usando fallback", e);
                    query_smart(&prompt, 80000).await
                }
            }
        } else {
            // Modo legacy: usa smart router diretamente
            let prompt = format!(
                "Tu √©s o Darwin RAG++ dentro do BEAGLE.

Pergunta do usu√°rio: {user_question}

Usa o knowledge graph inteiro (neo4j) + vector store (qdrant) + entity extraction.
Responde com racioc√≠nio estruturado + cita√ß√µes reais do graph.

Se n√£o souber, diz 'preciso de mais dados'."
            );

            info!("üîç GraphRAG query (legacy): {}", user_question);
            query_smart(&prompt, 80000).await
        }
    }

    /// Self-RAG real (o agente decide se precisa de mais busca)
    ///
    /// Sistema de gatekeeping que avalia confian√ßa da resposta:
    /// - Se confian√ßa < 85: gera nova query de busca
    /// - Se confian√ßa >= 85: retorna resposta atual
    pub async fn self_rag(&self, initial_answer: &str, question: &str) -> String {
        let check_prompt = format!(
            "Tu √©s o Self-RAG gatekeeper.

Pergunta original: {question}
Resposta atual: {initial_answer}

Score 0-100 de confian√ßa. Se <85, gera nova query de busca.
Responde JSON: {{\"confidence\": 88, \"new_query\": \"ou deixa vazio se ok\"}}"
        );

        info!("üéØ Self-RAG: avaliando confian√ßa da resposta");
        let gate = query_smart(&check_prompt, 10000).await;

        // Tenta parsear JSON da resposta
        let json: serde_json::Value = match serde_json::from_str(&gate) {
            Ok(v) => v,
            Err(e) => {
                warn!(
                    "‚ö†Ô∏è  Self-RAG: falha ao parsear JSON: {}. Retornando resposta original.",
                    e
                );
                return initial_answer.to_string();
            }
        };

        if let Some(conf) = json["confidence"].as_u64() {
            if conf < 85 {
                if let Some(new_q) = json["new_query"].as_str() {
                    if !new_q.is_empty() {
                        info!(
                            "üîÑ Self-RAG: confian√ßa {} < 85, buscando com nova query: {}",
                            conf, new_q
                        );
                        return self.graph_rag_query(new_q).await;
                    }
                }
            } else {
                info!("‚úÖ Self-RAG: confian√ßa {} >= 85, resposta aceita", conf);
            }
        }

        initial_answer.to_string()
    }

    /// Plugin system (troca LLM em runtime)
    ///
    /// Plugins dispon√≠veis:
    /// - `"grok3"`: Grok 3 via smart router (128k contexto, ilimitado)
    /// - `"local70b"`: vLLM local (Llama-3.3-70B-Instruct)
    /// - `"heavy"`: Grok 4.1 Heavy via smart router (256k contexto, quota)
    /// - `_`: Default para Grok 3
    pub async fn run_with_plugin(&self, prompt: &str, plugin: &str) -> String {
        match plugin {
            "grok3" => {
                info!("üîå Plugin: Grok 3 (128k contexto, ilimitado)");
                query_smart(prompt, 100000).await
            }
            "local70b" => {
                info!("üîå Plugin: vLLM local (Llama-3.3-70B-Instruct)");
                self.query_local_vllm(prompt).await
            }
            "heavy" => {
                info!("üîå Plugin: Grok 4.1 Heavy (256k contexto, quota)");
                // For√ßa uso do Heavy via smart router (contexto grande)
                query_smart(prompt, 200000).await
            }
            _ => {
                warn!("‚ö†Ô∏è  Plugin '{}' desconhecido, usando Grok 3", plugin);
                query_smart(prompt, 100000).await
            }
        }
    }

    /// Query vLLM local (helper interno)
    async fn query_local_vllm(&self, prompt: &str) -> String {
        let request = VllmCompletionRequest {
            model: "meta-llama/Llama-3.3-70B-Instruct".to_string(),
            prompt: prompt.to_string(),
            sampling_params: SamplingParams {
                temperature: 0.8,
                top_p: 0.95,
                max_tokens: 8192,
                n: 1,
                stop: None,
                frequency_penalty: 0.0,
            },
        };

        match self.vllm_client.completions(&request).await {
            Ok(response) => response
                .choices
                .first()
                .map(|c| c.text.trim().to_string())
                .unwrap_or_else(|| "Resposta vazia do vLLM".to_string()),
            Err(e) => {
                warn!("‚ùå Erro ao consultar vLLM local: {}", e);
                format!("Erro ao consultar vLLM local: {}", e)
            }
        }
    }
}

impl Default for DarwinCore {
    fn default() -> Self {
        Self::new()
    }
}

/// Ciclo completo Darwin-enhanced (GraphRAG + Self-RAG)
///
/// Pipeline:
/// 1. GraphRAG query inicial (usa hypergraph + neo4j + qdrant)
/// 2. Self-RAG avalia confian√ßa
/// 3. Se necess√°rio, busca adicional com nova query
/// 4. Retorna resposta final
///
/// # Example
/// ```rust
/// use beagle_darwin::darwin_enhanced_cycle;
///
/// let answer = darwin_enhanced_cycle("unificar entropia curva com consci√™ncia celular").await;
/// println!("DARWIN + BEAGLE: {answer}");
/// ```
pub async fn darwin_enhanced_cycle(question: &str) -> String {
    info!("üöÄ Darwin Enhanced Cycle iniciado: {}", question);

    let darwin = DarwinCore::new();

    // 1. GraphRAG query inicial
    let initial = darwin.graph_rag_query(question).await;

    // 2. Self-RAG avalia e potencialmente busca mais
    let final_answer = darwin.self_rag(&initial, question).await;

    info!("‚úÖ Darwin Enhanced Cycle conclu√≠do");
    final_answer
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_darwin_core_creation() {
        let darwin = DarwinCore::new();
        assert!(darwin.graph_rag_enabled);
        assert!(darwin.self_rag_enabled);
    }

    #[tokio::test]
    async fn test_plugin_system() {
        let darwin = DarwinCore::new();
        // Testa que o plugin system n√£o quebra (pode falhar se n√£o tiver LLM configurado)
        let _result = darwin.run_with_plugin("Test prompt", "grok3").await;
        // N√£o asserta sucesso pois pode n√£o ter API keys configuradas nos testes
    }
}
