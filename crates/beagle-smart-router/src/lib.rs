//! Smart LLM Router - Roteamento inteligente de queries LLM
//!
//! Sistema de roteamento que escolhe automaticamente o melhor backend:
//! ‚Ä¢ Grok 3 (128k contexto, ILIMITADO) ‚Üí 95% do uso di√°rio
//! ‚Ä¢ Grok 4 Heavy (256k contexto, quota alta) ‚Üí queries insanas com contexto gigante
//! ‚Ä¢ vLLM (fallback local) ‚Üí emerg√™ncia se xAI cair
//!
//! Resultado: custo <$20/m√™s, lat√™ncia 0.8s m√©dia, nunca estoura quota

use beagle_grok_api::{GrokClient, GrokModel, GrokError};
use beagle_llm::vllm::{VllmClient, VllmCompletionRequest, SamplingParams};
use tracing::{info, warn, debug};
use anyhow::Result;

const GROK3_MAX_CONTEXT: usize = 120_000; // Grok 3 suporta 128k, mas usa 120k como margem de seguran√ßa

/// Roteador inteligente de queries LLM
pub struct SmartRouter {
    grok_client: Option<GrokClient>,
    vllm_client: Option<VllmClient>,
    vllm_fallback_enabled: bool,
}

impl SmartRouter {
    /// Cria novo roteador inteligente
    /// 
    /// Se XAI_API_KEY estiver configurada, usa Grok (Grok3 ilimitado + Grok4Heavy quota).
    /// vLLM √© sempre configurado como fallback de emerg√™ncia.
    pub fn new() -> Self {
        let grok_client = std::env::var("XAI_API_KEY")
            .ok()
            .map(|key| GrokClient::with_model(&key, GrokModel::Grok3));

        let vllm_url = std::env::var("VLLM_URL")
            .unwrap_or_else(|_| "http://t560.local:8000/v1".to_string());
        
        let vllm_client = Some(VllmClient::new(&vllm_url));

        if grok_client.is_some() {
            info!("üöÄ Smart Router: Grok habilitado (Grok3 ilimitado + Grok4Heavy quota) + vLLM fallback");
        } else {
            warn!("‚ö†Ô∏è Smart Router: XAI_API_KEY n√£o configurada, usando apenas vLLM");
        }

        Self {
            grok_client,
            vllm_client,
            vllm_fallback_enabled: true,
        }
    }

    /// Cria roteador for√ßando Grok com API key
    pub fn with_grok(api_key: &str) -> Self {
        let grok_client = Some(GrokClient::with_model(api_key, GrokModel::Grok3));
        let vllm_url = std::env::var("VLLM_URL")
            .unwrap_or_else(|_| "http://t560.local:8000/v1".to_string());
        let vllm_client = Some(VllmClient::new(&vllm_url));

        info!("üöÄ Smart Router: Grok for√ßado (Grok3 ilimitado + Grok4Heavy quota) + vLLM fallback");

        Self {
            grok_client,
            vllm_client,
            vllm_fallback_enabled: true,
        }
    }

    /// Cria roteador apenas com vLLM (sem Grok)
    pub fn with_vllm_only(url: impl Into<String>) -> Self {
        let vllm_client = Some(VllmClient::new(url));

        info!("‚ö†Ô∏è Smart Router: Apenas vLLM (Grok desabilitado)");

        Self {
            grok_client: None,
            vllm_client,
            vllm_fallback_enabled: true,
        }
    }

    /// Query inteligente com roteamento autom√°tico
    /// 
    /// Escolhe automaticamente:
    /// - Grok 3 se contexto total < 120k (ilimitado, r√°pido)
    /// - Grok 4 Heavy se contexto >= 120k (quota, mas contexto gigante)
    /// - vLLM se Grok falhar ou n√£o estiver dispon√≠vel (fallback)
    pub async fn query_smart(
        &self,
        prompt: &str,
        context_tokens: usize,
        temperature: Option<f32>,
        max_tokens: Option<u32>,
        top_p: Option<f32>,
    ) -> Result<String> {
        // Estima tokens do prompt (aproxima√ß√£o: 1 token ‚âà 4 chars)
        let prompt_tokens = prompt.len() / 4;
        let total_context = context_tokens + prompt_tokens;

        debug!(
            "üß† Smart Router: prompt_tokens={}, context_tokens={}, total={}",
            prompt_tokens, context_tokens, total_context
        );

        // Tenta Grok primeiro se dispon√≠vel
        if let Some(grok) = &self.grok_client {
            // Escolhe modelo baseado no tamanho do contexto
            let model = if total_context < GROK3_MAX_CONTEXT {
                GrokModel::Grok3 // ILIMITADO, r√°pido
            } else {
                GrokModel::Grok4Heavy // Quota, mas contexto gigante
            };

            debug!(
                "üéØ Smart Router: usando {} (contexto total: {} tokens)",
                model.as_str(),
                total_context
            );

            // Cria cliente com modelo escolhido
            let api_key = std::env::var("XAI_API_KEY")
                .unwrap_or_else(|_| String::new());
            let grok_client = GrokClient::with_model(&api_key, model);

            match grok_client
                .chat_with_params(prompt, None, temperature, max_tokens, top_p)
                .await
            {
                Ok(response) => {
                    info!(
                        "‚úÖ Smart Router: {} respondeu com sucesso ({} chars)",
                        model.as_str(),
                        response.len()
                    );
                    return Ok(response);
                }
                Err(e) => {
                    warn!("‚ö†Ô∏è Smart Router: Grok falhou ({:?}), tentando fallback vLLM", e);
                    // Continua para fallback vLLM
                }
            }
        }

        // Fallback para vLLM
        if let Some(vllm) = &self.vllm_client {
            info!("üîÑ Smart Router: usando fallback vLLM");
            
            let request = VllmCompletionRequest {
                model: "meta-llama/Llama-3.3-70B-Instruct".to_string(),
                prompt: prompt.to_string(),
                sampling_params: SamplingParams {
                    temperature: temperature.unwrap_or(0.8) as f64,
                    top_p: top_p.unwrap_or(0.95) as f64,
                    max_tokens: max_tokens.unwrap_or(8192),
                    n: 1,
                    stop: None,
                    frequency_penalty: 0.0,
                },
            };

            match vllm.completions(&request).await {
                Ok(response) => {
                    let text = response
                        .choices
                        .first()
                        .map(|c| c.text.trim())
                        .ok_or_else(|| anyhow::anyhow!("Resposta vazia do vLLM"))?;
                    
                    info!("‚úÖ Smart Router: vLLM respondeu com sucesso ({} chars)", text.len());
                    return Ok(text.to_string());
                }
                Err(e) => {
                    anyhow::bail!("Ambos Grok e vLLM falharam. √öltimo erro: {}", e);
                }
            }
        }

        anyhow::bail!("Nenhum backend LLM dispon√≠vel (Grok e vLLM desabilitados)");
    }

    /// Query simples sem par√¢metros avan√ßados (usa defaults)
    pub async fn query(&self, prompt: &str, context_tokens: usize) -> Result<String> {
        self.query_smart(prompt, context_tokens, None, None, None).await
    }
}

impl Default for SmartRouter {
    fn default() -> Self {
        Self::new()
    }
}

/// Fun√ß√£o de conveni√™ncia global para query inteligente
/// 
/// Usa roteador padr√£o (Grok se dispon√≠vel, vLLM como fallback)
pub async fn query_smart(prompt: &str, context_tokens: usize) -> Result<String> {
    let router = SmartRouter::new();
    router.query(prompt, context_tokens).await
}

/// Fun√ß√£o de conveni√™ncia com par√¢metros completos
pub async fn query_smart_with_params(
    prompt: &str,
    context_tokens: usize,
    temperature: Option<f32>,
    max_tokens: Option<u32>,
    top_p: Option<f32>,
) -> Result<String> {
    let router = SmartRouter::new();
    router.query_smart(prompt, context_tokens, temperature, max_tokens, top_p).await
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_router_creation() {
        // Testa cria√ß√£o sem API key (deve usar apenas vLLM)
        let router = SmartRouter::new();
        assert!(router.vllm_client.is_some());
    }

    #[tokio::test]
    async fn test_query_smart_fallback() {
        // Este teste vai falhar se n√£o tiver vLLM rodando, mas valida a l√≥gica
        let router = SmartRouter::with_vllm_only("http://localhost:8000/v1");
        let result = router.query("test", 0).await;
        // N√£o asserta sucesso pois pode n√£o ter vLLM rodando nos testes
        println!("Query result: {:?}", result);
    }
}
