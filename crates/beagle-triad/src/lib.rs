//! beagle-triad - Honest AI Triad
//!
//! Sistema adversarial de revis√£o:
//! - ATHENA: agente "literatura" (pontos fortes/fracos, sugest√µes)
//! - HERMES: revisor (reescreve mantendo estilo/autoria)
//! - ARGOS: cr√≠tico (falhas l√≥gicas, claims sem suporte)
//! - Juiz final: arbitra vers√µes finais

use beagle_core::BeagleContext;
use beagle_llm::{RequestMeta, ProviderTier, stats::LlmCallsStats as LlmCallsStatsLLM};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use tracing::{info, warn};

/// Gera resumo simb√≥lico do draft usando PCS (Symbolic Computational Psychiatry)
/// Extrai conceitos-chave, rela√ß√µes l√≥gicas e estrutura sem√¢ntica
pub async fn generate_symbolic_summary(draft: &str, ctx: &BeagleContext) -> anyhow::Result<String> {
    info!("Gerando resumo simb√≥lico do draft");
    
    // Por enquanto, usa heur√≠sticas simples para extrair conceitos
    // TODO: Integrar com PCS real via Julia quando dispon√≠vel
    let concepts = extract_key_concepts(draft);
    let logical_structure = analyze_logical_structure(draft);
    
    let summary = format!(
        "## Resumo Simb√≥lico (PCS)\n\n\
        **Conceitos-chave**: {}\n\n\
        **Estrutura l√≥gica**: {}\n\n\
        **Nota**: Este resumo foi gerado usando heur√≠sticas b√°sicas. \
        Integra√ß√£o completa com PCS Symbolic Psychiatry ser√° implementada via Julia.",
        concepts.join(", "),
        logical_structure
    );
    
    Ok(summary)
}

fn extract_key_concepts(text: &str) -> Vec<String> {
    // Heur√≠stica simples: palavras em mai√∫sculas, termos t√©cnicos comuns
    let keywords = [
        "entropia", "curvatura", "scaffold", "biomaterial", "PBPK", "KEC",
        "psiquiatria", "computacional", "neuroci√™ncia", "filosofia", "consci√™ncia",
        "geometria", "n√£o-comutativa", "fractal", "hologr√°fico"
    ];
    
    let mut found = Vec::new();
    let text_lower = text.to_lowercase();
    
    for keyword in &keywords {
        if text_lower.contains(keyword) {
            found.push(keyword.to_string());
        }
    }
    
    found
}

fn analyze_logical_structure(text: &str) -> String {
    // Heur√≠stica simples: conta se√ß√µes, refer√™ncias, equa√ß√µes
    let sections = text.matches("##").count();
    let references = text.matches("@").count() + text.matches("\\cite").count();
    let equations = text.matches("$$").count() / 2; // pares
    
    format!(
        "{} se√ß√µes principais, {} refer√™ncias, {} equa√ß√µes",
        sections, references, equations
    )
}

/// Input para a Triad
#[derive(Debug, Clone)]
pub struct TriadInput {
    pub run_id: String,
    pub draft_path: PathBuf,
    pub context_summary: Option<String>, // pode ser JSON com top-k chunks, etc.
}

/// Opini√£o de um agente da Triad
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TriadOpinion {
    pub agent: String,      // "ATHENA" | "HERMES" | "ARGOS"
    pub summary: String,
    pub suggestions_md: String, // markdown
    pub score: f32,         // 0.0‚Äì1.0
    pub provider_tier: String, // "grok-3" | "grok-4-heavy" | etc.
}

/// Relat√≥rio final da Triad
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TriadReport {
    pub run_id: String,
    pub original_draft: String,
    pub final_draft: String,
    pub opinions: Vec<TriadOpinion>,
    pub created_at: DateTime<Utc>,
    pub llm_stats: LlmCallsStatsLLM,
}

/// Estat√≠sticas de chamadas LLM
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct LlmCallsStats {
    pub grok3_calls: usize,
    pub grok3_tokens_est: usize,
    pub heavy_calls: usize,
    pub heavy_tokens_est: usize,
}

/// Executa a Triad completa
pub async fn run_triad(
    input: &TriadInput,
    ctx: &BeagleContext,
) -> anyhow::Result<TriadReport> {
    info!("üîç Iniciando Triad para run_id: {}", input.run_id);

    // 1) Ler draft
    let original_draft = std::fs::read_to_string(&input.draft_path)?;
    info!("üìÑ Draft lido: {} chars", original_draft.len());

    // 2) ATHENA (agente literatura)
    info!("üî¨ Executando ATHENA...");
    let (athena, tier) = run_athena(&original_draft, &input.context_summary, ctx, &input.run_id).await?;
    info!("‚úÖ ATHENA conclu√≠do - Score: {:.2} | Provider: {}", athena.score, tier.as_str());

    // 3) HERMES (revisor)
    info!("‚úçÔ∏è  Executando HERMES...");
    let (hermes, tier) = run_hermes(&original_draft, &athena, ctx, &input.run_id).await?;
    info!("‚úÖ HERMES conclu√≠do - Score: {:.2} | Provider: {}", hermes.score, tier.as_str());

    // 4) ARGOS (cr√≠tico)
    info!("‚öîÔ∏è  Executando ARGOS...");
    let (argos, tier) = run_argos(&original_draft, &hermes, &athena, ctx, &input.run_id).await?;
    info!("‚úÖ ARGOS conclu√≠do - Score: {:.2} | Provider: {}", argos.score, tier.as_str());

    // 5) Juiz final (arbitra vers√µes)
    info!("‚öñÔ∏è  Executando Juiz Final...");
    let (final_draft, tier) = arbitrate_final(
        &original_draft,
        &hermes,
        &athena,
        &argos,
        ctx,
        &input.run_id,
    )
    .await?;
    info!("‚úÖ Juiz Final conclu√≠do - Draft final: {} chars | Provider: {}", final_draft.len(), tier.as_str());

    // Obt√©m stats finais do contexto
    let llm_stats = ctx.llm_stats.get(&input.run_id).unwrap_or_default();
    let llm_stats_converted = LlmCallsStatsLLM {
        grok3_calls: llm_stats.grok3_calls,
        grok3_tokens_in: llm_stats.grok3_tokens_in,
        grok3_tokens_out: llm_stats.grok3_tokens_out,
        grok4_calls: llm_stats.grok4_calls,
        grok4_tokens_in: llm_stats.grok4_tokens_in,
        grok4_tokens_out: llm_stats.grok4_tokens_out,
    };

    Ok(TriadReport {
        run_id: input.run_id.clone(),
        original_draft,
        final_draft,
        opinions: vec![athena, hermes, argos],
        created_at: Utc::now(),
        llm_stats: llm_stats_converted,
    })
}

/// Helper para chamada LLM com tracking de stats na Triad
async fn call_llm_with_stats_triad(
    ctx: &BeagleContext,
    run_id: &str,
    prompt: &str,
    meta: RequestMeta,
) -> anyhow::Result<(String, ProviderTier)> {
    // Obt√©m stats atuais do run
    let current_stats = ctx.llm_stats.get_or_create(run_id);
    
    // Escolhe client com limites
    let (client, tier) = ctx.router.choose_with_limits(&meta, &current_stats);
    
    // Chama LLM
    let output = client.complete(prompt).await?;
    
    // Atualiza stats
    ctx.llm_stats.update(run_id, |stats| {
        match tier {
            ProviderTier::Grok3 => {
                stats.grok3_calls += 1;
                stats.grok3_tokens_in += output.tokens_in_est as u32;
                stats.grok3_tokens_out += output.tokens_out_est as u32;
            }
            ProviderTier::Grok4Heavy => {
                stats.grok4_calls += 1;
                stats.grok4_tokens_in += output.tokens_in_est as u32;
                stats.grok4_tokens_out += output.tokens_out_est as u32;
            }
            _ => {
                // Outros tiers contam como Grok3 por enquanto
                stats.grok3_calls += 1;
                stats.grok3_tokens_in += output.tokens_in_est as u32;
                stats.grok3_tokens_out += output.tokens_out_est as u32;
            }
        }
    });
    
    Ok((output.text, tier))
}

/// ATHENA: leitura cr√≠tica + literatura
/// 
/// Prompts customizados para o contexto cient√≠fico interdisciplinar do BEAGLE:
/// - Psiquiatria computacional, entropia/curvatura, PBPK, biomateriais, neuroci√™ncia
/// - Filosofia da mente, geometria n√£o-comutativa, consci√™ncia celular
pub async fn run_athena(
    draft: &str,
    context_summary: &Option<String>,
    ctx: &BeagleContext,
    run_id: &str,
) -> anyhow::Result<(TriadOpinion, ProviderTier)> {
    let mut prompt = String::from(
        "Voc√™ √© ATHENA, agente de leitura cr√≠tica e contexto cient√≠fico do sistema BEAGLE.\n\n\
        O contexto de pesquisa envolve √°reas interdisciplinares:\n\
        - Psiquiatria computacional e neuroci√™ncia\n\
        - Entropia curva e geometria n√£o-comutativa\n\
        - Modelagem PBPK (Farmacocin√©tica Fisiol√≥gica) e KEC\n\
        - Biomateriais e scaffolds biol√≥gicos\n\
        - Consci√™ncia celular e filosofia da mente\n\
        - Engenharia qu√≠mica aplicada a sistemas biol√≥gicos\n\n\
        Analise o rascunho de artigo abaixo, identifique:\n\
        - Pontos fortes conceituais (especialmente conex√µes interdisciplinares)\n\
        - Fragilidades metodol√≥gicas ou conceituais\n\
        - Refer√™ncias/literatura adicional relevante nestas √°reas (Nature, Kybernetes, Frontiers, etc.)\n\n\
        Responda em tr√™s se√ß√µes Markdown: ## Pontos Fortes, ## Fragilidades, ## Refer√™ncias Sugeridas.\n\n",
    );

    if let Some(ctx_sum) = context_summary {
        prompt.push_str("=== CONTEXTO (Darwin / GraphRAG) ===\n");
        prompt.push_str(ctx_sum);
        prompt.push_str("\n\n");
    }

    // Adiciona contexto simb√≥lico se habilitado (via env ou config)
    if std::env::var("BEAGLE_SYMBOLIC_CONTEXT_ENABLE")
        .unwrap_or_else(|_| "false".to_string())
        .parse::<bool>()
        .unwrap_or(false)
    {
        if let Ok(symbolic_summary) = generate_symbolic_summary(draft, ctx).await {
            prompt.push_str("=== CONTEXTO SIMB√ìLICO (PCS) ===\n");
            prompt.push_str(&symbolic_summary);
            prompt.push_str("\n\n");
        }
    }

    prompt.push_str("=== DRAFT ===\n");
    prompt.push_str(draft);

    let meta = RequestMeta::new(
        false, // requires_math
        true,  // requires_high_quality
        false, // offline_required
        prompt.chars().count() / 4, // approximate_tokens
        false, // high_bias_risk (ATHENA n√£o precisa de Heavy normalmente)
        true,  // requires_phd_level_reasoning (avalia ci√™ncia)
        false, // critical_section
    );

    let (text, tier) = call_llm_with_stats_triad(ctx, run_id, &prompt, meta).await?;

    // Extrai score (pode pedir ao modelo explicitamente no futuro)
    let score = extract_score(&text).unwrap_or(0.8);

    Ok((
        TriadOpinion {
            agent: "ATHENA".into(),
            summary: "Leitura cr√≠tica e mapeamento de literatura sugerida".into(),
            suggestions_md: text,
            score,
            provider_tier: tier.as_str().to_string(),
        },
        tier,
    ))
}

/// HERMES: reescrita orientada
/// 
/// Preserva voz autoral interdisciplinar (engenharia qu√≠mica, medicina, psiquiatria, biomateriais, filosofia da mente).
/// Alta densidade conceitual sem simplifica√ß√£o infantil.
pub async fn run_hermes(
    draft: &str,
    athena: &TriadOpinion,
    ctx: &BeagleContext,
    run_id: &str,
) -> anyhow::Result<(TriadOpinion, ProviderTier)> {
    let mut prompt = String::from(
        "Voc√™ √© HERMES, agente de s√≠ntese textual do sistema BEAGLE.\n\n\
        IMPORTANTE: Preserve a voz autoral interdisciplinar caracter√≠stica de um pesquisador que trabalha na intersec√ß√£o de:\n\
        - Engenharia qu√≠mica e farmacocin√©tica (PBPK)\n\
        - Medicina e psiquiatria computacional\n\
        - Biomateriais e scaffolds biol√≥gicos\n\
        - Neuroci√™ncia e filosofia da mente\n\
        - Geometria n√£o-comutativa e entropia curva\n\n\
        Mantenha alta densidade conceitual, clareza sem simplifica√ß√£o infantil, eleg√¢ncia t√©cnica.\n\n\
        Voc√™ receber√°:\n\
        - Um rascunho de artigo (DRAFT)\n\
        - Uma an√°lise cr√≠tica de ATHENA com sugest√µes (ATHENA_FEEDBACK)\n\n\
        Sua tarefa:\n\
        1. Reescrever o texto deixando-o mais claro, coeso e l√≥gico.\n\
        2. Incorporar as sugest√µes relevantes de ATHENA.\n\
        3. N√ÉO inventar dados ou resultados; s√≥ reorganizar e melhorar o texto.\n\
        4. Mantenha o rigor t√©cnico e a voz autoral interdisciplinar.\n\n\
        Responda apenas com o novo texto em Markdown, sem coment√°rios fora do texto.\n\n",
    );

    prompt.push_str("=== ATHENA_FEEDBACK ===\n");
    prompt.push_str(&athena.suggestions_md);
    prompt.push_str("\n\n=== DRAFT ===\n");
    prompt.push_str(draft);

    let meta = RequestMeta::new(
        false, // requires_math
        true,  // requires_high_quality
        false, // offline_required
        prompt.chars().count() / 4, // approximate_tokens
        false, // high_bias_risk (HERMES n√£o precisa de Heavy)
        false, // requires_phd_level_reasoning (reescrita, n√£o an√°lise cr√≠tica)
        false, // critical_section
    );

    let (text, tier) = call_llm_with_stats_triad(ctx, run_id, &prompt, meta).await?;

    let score = extract_score(&text).unwrap_or(0.85);

    Ok((
        TriadOpinion {
            agent: "HERMES".into(),
            summary: "Reescrita coerente e estilisticamente melhorada".into(),
            suggestions_md: text.clone(), // aqui o 'suggestions_md' √© o pr√≥prio rascunho reescrito
            score,
            provider_tier: tier.as_str().to_string(),
        },
        tier,
    ))
}

/// ARGOS: cr√≠tico adversarial
/// 
/// Age como revisor Q1 duro (Nature Human Behaviour, Kybernetes, Frontiers), focado em:
/// - Claims sem suporte emp√≠rico adequado
/// - Confus√£o entre met√°fora e mecanismo
/// - Aus√™ncia de desenho emp√≠rico razo√°vel
pub async fn run_argos(
    original_draft: &str,
    hermes: &TriadOpinion,
    athena: &TriadOpinion,
    ctx: &BeagleContext,
    run_id: &str,
) -> anyhow::Result<(TriadOpinion, ProviderTier)> {
    let mut prompt = String::from(
        "Voc√™ √© ARGOS, agente cr√≠tico adversarial do sistema BEAGLE.\n\n\
        Voc√™ atua como revisor Q1 rigoroso (Nature Human Behaviour, Kybernetes, Frontiers in Computational Neuroscience).\n\
        Foque especialmente em:\n\
        - Claims sem suporte emp√≠rico adequado (extrapola√ß√µes n√£o suportadas)\n\
        - Confus√£o entre met√°fora po√©tica e mecanismo cient√≠fico concreto\n\
        - Aus√™ncia de desenho emp√≠rico razo√°vel (onde h√° espa√ßo para experimentos/predictions test√°veis)\n\
        - Problemas de coer√™ncia l√≥gica e ambiguidade conceitual\n\n\
        Voc√™ recebeu:\n\
        - O DRAFT original de um artigo\n\
        - Um DRAFT reescrito por HERMES\n\
        - Coment√°rios de ATHENA\n\n\
        Sua fun√ß√£o:\n\
        1. Liste problemas graves de coer√™ncia l√≥gica, extrapola√ß√µes n√£o suportadas, ambiguidade.\n\
        2. Aponte onde HERMES melhorou o texto e onde piorou.\n\
        3. Sugira corre√ß√µes pontuais (especialmente onde o texto precisa ser mais rigoroso cientificamente).\n\n\
        Responda em Markdown com se√ß√µes: ## Problemas Graves, ## Melhorias de HERMES, ## Sugest√µes Pontuais.\n\n",
    );

    prompt.push_str("=== ATHENA_FEEDBACK ===\n");
    prompt.push_str(&athena.suggestions_md);
    prompt.push_str("\n\n=== DRAFT_ORIGINAL ===\n");
    prompt.push_str(original_draft);
    prompt.push_str("\n\n=== DRAFT_HERMES ===\n");
    prompt.push_str(&hermes.suggestions_md);

    // ARGOS usa Heavy: cr√≠tica sobre claims cient√≠ficos
    let meta = RequestMeta::new(
        false, // requires_math (ou true se for Methods de KEC/PBPK)
        true,  // requires_high_quality
        false, // offline_required
        prompt.chars().count() / 4, // approximate_tokens
        true,  // high_bias_risk (cr√≠tica sobre claims cient√≠ficos)
        true,  // requires_phd_level_reasoning
        true,  // critical_section (revis√£o cr√≠tica)
    );

    let (text, tier) = call_llm_with_stats_triad(ctx, run_id, &prompt, meta).await?;

    let score = extract_score(&text).unwrap_or(0.9);

    Ok((
        TriadOpinion {
            agent: "ARGOS".into(),
            summary: "Cr√≠tica adversarial e apontamento de falhas l√≥gicas".into(),
            suggestions_md: text,
            score,
            provider_tier: tier.as_str().to_string(),
        },
        tier,
    ))
}

/// Juiz final: arbitragem do draft
/// 
/// Combina o melhor dos tr√™s agentes (ATHENA/HERMES/ARGOS) mantendo rigor cient√≠fico e estilo interdisciplinar.
/// Foca em resolver problemas cr√≠ticos apontados por ARGOS enquanto preserva a voz autoral.
pub async fn arbitrate_final(
    original_draft: &str,
    hermes: &TriadOpinion,
    athena: &TriadOpinion,
    argos: &TriadOpinion,
    ctx: &BeagleContext,
    run_id: &str,
) -> anyhow::Result<(String, ProviderTier)> {
    // Gera resumo simb√≥lico (PCS) do draft original
    let symbolic_summary = generate_symbolic_summary(original_draft, ctx).await
        .unwrap_or_else(|e| {
            warn!("Falha ao gerar resumo simb√≥lico: {}", e);
            "Resumo simb√≥lico n√£o dispon√≠vel".to_string()
        });
    
    let mut prompt = String::from(
        "Voc√™ √© o JUIZ FINAL do sistema BEAGLE (HONEST AI TRIAD).\n\n\
        IMPORTANTE: Mantenha a voz autoral interdisciplinar (engenharia qu√≠mica, medicina, psiquiatria, biomateriais, filosofia da mente).\n\
        Preserve alta densidade conceitual e eleg√¢ncia t√©cnica.\n\n\
        **Resumo Simb√≥lico (PCS)**:\n{}\n\n\
        Voc√™ recebeu:\n\
        - DRAFT_ORIGINAL: rascunho original do artigo.\n\
        - DRAFT_HERMES: vers√£o reescrita por HERMES.\n\
        - FEEDBACK_ATHENA: an√°lise cr√≠tica e sugest√µes de literatura.\n\
        - FEEDBACK_ARGOS: cr√≠tica adversarial rigorosa (n√≠vel Q1).\n\n\
        Sua tarefa:\n\
        1. Produzir uma vers√£o FINAL do texto, em Markdown, incorporando o melhor de cada um.\n\
        2. Corrigir problemas graves apontados por ARGOS (claims sem suporte, confus√£o met√°fora/mecanismo, etc.).\n\
        3. Incorporar sugest√µes relevantes de ATHENA quando apropriado.\n\
        4. Manter a voz autoral interdisciplinar e evitar inventar dados.\n\n\
        Responda **apenas** com o texto final em Markdown.\n\n",
    );
    prompt = prompt.replace("{}", &symbolic_summary);

    prompt.push_str("=== FEEDBACK_ATHENA ===\n");
    prompt.push_str(&athena.suggestions_md);
    prompt.push_str("\n\n=== FEEDBACK_ARGOS ===\n");
    prompt.push_str(&argos.suggestions_md);
    
    // Contexto simb√≥lico j√° foi adicionado no in√≠cio do prompt
    
    prompt.push_str("\n\n=== DRAFT_ORIGINAL ===\n");
    prompt.push_str(original_draft);
    prompt.push_str("\n\n=== DRAFT_HERMES ===\n");
    prompt.push_str(&hermes.suggestions_md);

    // Juiz Final usa Heavy: decis√£o final sobre texto cient√≠fico
    let meta = RequestMeta::new(
        false, // requires_math
        true,  // requires_high_quality
        false, // offline_required
        prompt.chars().count() / 4, // approximate_tokens
        true,  // high_bias_risk (decis√£o final sobre texto cient√≠fico)
        true,  // requires_phd_level_reasoning
        true,  // critical_section (vers√£o final)
    );

    call_llm_with_stats_triad(ctx, run_id, &prompt, meta).await
}

// update_stats removido - agora usa call_llm_with_stats_triad que atualiza ctx.llm_stats diretamente

/// Extrai score de resposta (simplificado)
fn extract_score(response: &str) -> Option<f32> {
    // Procura por padr√µes como "Score: 0.85" ou "0.85"
    let re = regex::Regex::new(r"score[:\s]+([0-9]+\.[0-9]+)").ok()?;
    let binding = response.to_lowercase();
    let caps = re.captures(binding.as_str())?;
    caps.get(1)?.as_str().parse().ok()
}
