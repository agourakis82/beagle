# ğŸ”¬ BEAGLE-JULIA - Quantum-Inspired Reasoning Engine

**MigraÃ§Ã£o do BEAGLE para Julia 1.10+** - Performance de C com sintaxe elegante.

## ğŸš€ Por que Julia?

- **50-100x mais rÃ¡pido que Python** em loops numÃ©ricos
- **Sintaxe limpa e expressiva** (melhor que R, mais legÃ­vel que C++)
- **Tipagem estÃ¡tica opcional** (performance quando precisa, flexibilidade quando nÃ£o)
- **Ecosistema ML maduro** (Flux, Lux, Zygote para LoRA no M3 Max)
- **Metaprogramming poderoso** (perfeito para fractais recursivos)

## ğŸ“¦ Setup RÃ¡pido (10 minutos)

### 1. Instalar Julia 1.10.5

```bash
# Linux/WSL
curl -fsSL https://install.julialang.org | sh

# macOS (via Homebrew)
brew install julia

# Ou baixar direto: https://julialang.org/downloads/
```

### 2. Criar e Ativar Projeto

```bash
cd beagle-julia
julia --project=.
```

### 3. Instalar DependÃªncias

No REPL Julia:

```julia
using Pkg
Pkg.activate(".")
Pkg.instantiate()  # Instala todas as dependÃªncias do Project.toml
```

Ou manualmente:

```julia
]
add Random LinearAlgebra Statistics
add DataFrames DataFramesMeta
add HTTP JSON3 PythonCall
add Flux Lux Zygote
add Plots Makie
add UUIDs Logging BenchmarkTools
```

### 4. Setup LoRA Training (Opcional)

Se quiser usar LoRA training com Unsloth:

```bash
bash setup_lora.sh
```

Isso instala Unsloth com suporte CUDA automÃ¡tico detectado (Ampere/Hopper).

## ğŸ¯ Uso BÃ¡sico

### Quantum Reasoning

```julia
using BeagleQuantum

# Criar conjunto de hipÃ³teses
set = HypothesisSet()

# Adicionar hipÃ³teses
add!(set, "Entropia curva Ã© geomÃ©trica")
add!(set, "Entropia curva Ã© quÃ¢ntica de campo")
add!(set, "Entropia curva Ã© consciÃªncia celular")

# Aplicar interferÃªncia com evidÃªncia
interference!(set, "evidÃªncia aponta pra consciÃªncia celular", 1.5)

# Colapsar superposiÃ§Ã£o
result = collapse(set, strategy=:probabilistic)
println(result)

# Ou rodar demo completa
demo()
```

### Multi-Agent Orchestrator

```julia
# Carregar ambos os mÃ³dulos (adversarial Ã© usado automaticamente se score < 98%)
include("adversarial.jl")
include("orchestrator.jl")
using BeagleOrchestrator

# Rodar pipeline completo: ATHENA â†’ Quantum â†’ HERMES â†’ ARGOS â†’ Adversarial
research = "Unificar entropia curva em scaffolds biolÃ³gicos com consciÃªncia celular via geometria nÃ£o-comutativa"
orch = Orchestrator(research)
final_draft = run_cycle!(orch)

# Ou usar funÃ§Ã£o main()
main()
```

O orchestrator executa em **<30s** com cluster vLLM local. Se ARGOS score < 98%, ativa automaticamente o loop adversarial para refinamento iterativo atÃ© quality â‰¥ 98.5%.

### Adversarial Self-Play

```julia
include("adversarial.jl")
using BeagleAdversarial

# Loop adversarial standalone
context = "Entropia curva em scaffolds biolÃ³gicos Ã© mediada por consciÃªncia celular via geometria nÃ£o-comutativa"
final_draft = adversarial_self_play(context, max_iters=6, target_quality=98.5)

# Ou rodar teste rÃ¡pido
test()
```

O adversarial loop itera atÃ© quality â‰¥ 98.5% ou max_iters, refinando draft via ARGOS â†’ HERMES.

### LoRA Training Real

```julia
include("lora_training.jl")
using BeagleLoRATraining

# Dados do adversarial loop (bad â†’ good drafts)
bad_drafts = ["draft ruim 1", "draft ruim 2"]
good_drafts = ["draft bom 1", "draft bom 2"]
contexts = ["contexto 1", "contexto 2"]

# Pipeline completo: dataset â†’ load â†’ train â†’ save
adapter_path = BeagleLoRATraining.full_training_pipeline(
    bad_drafts, good_drafts, contexts;
    hf_token=nothing,  # ou teu token HuggingFace
    max_steps=60,
    output_dir="beagle_lora_adapter"
)

# Adversarial com LoRA training automÃ¡tico
include("adversarial.jl")
using BeagleAdversarial

final_draft = adversarial_self_play(
    "Entropia curva em scaffolds biolÃ³gicos...",
    enable_lora_training=true,  # Ativa treinamento incremental
    hf_token=nothing,
    lora_output_dir="beagle_lora_adapter"
)

# Usar adapter no vLLM:
# vllm serve meta-llama/Llama-3.3-70B-Instruct --lora-path beagle_lora_adapter
```

LoRA training executa em **5-10 minutos** no cluster com GPU, salvando adapter GGUF pronto para vLLM imediato.

## ğŸ“š Estrutura do MÃ³dulo

### `Hypothesis`
- `content::String` - ConteÃºdo da hipÃ³tese
- `amplitude::ComplexF64` - Amplitude complexa (como funÃ§Ã£o de onda)
- `probability::Float64` - Probabilidade = |amplitude|Â²
- `phase::Float64` - Fase (para interferÃªncia)
- `evidence_count::Int` - Contador de evidÃªncias

### `HypothesisSet`
- `hyps::Vector{Hypothesis}` - Vetor de hipÃ³teses em superposiÃ§Ã£o
- `is_collapsed::Bool` - Flag de colapso

### `InterferenceEngine`
- `coupling_strength::Float64` - ForÃ§a de acoplamento
- `interference!()` - InterferÃªncia baseada em evidÃªncia textual
- `apply_constructive_interference!()` - InterferÃªncia construtiva
- `apply_destructive_interference!()` - InterferÃªncia destrutiva

### `MeasurementOperator`
- `threshold::Float64` - Threshold de probabilidade
- `measure()` - Colapsa superposiÃ§Ã£o se threshold atingido

### `Orchestrator` (Multi-Agent)
- `research_question::String` - Pergunta de pesquisa
- `run_cycle!()` - Executa pipeline completo:
  1. **ATHENA**: RevisÃ£o bibliogrÃ¡fica + identificaÃ§Ã£o de gaps
  2. **Quantum Superposition**: 6 hipÃ³teses paralelas com interferÃªncia
  3. **HERMES**: GeraÃ§Ã£o de draft preservando voz autoral
  4. **ARGOS**: CrÃ­tica adversarial com score de qualidade
  5. **Adversarial Loop**: Refinamento iterativo se score < 98%

### `BeagleAdversarial` (Adversarial Self-Play)
- `adversarial_self_play(context; max_iters=6, target_quality=98.5, enable_lora_training=false, ...)` - Loop iterativo:
  1. **HERMES**: Gera draft inicial
  2. **ARGOS**: Avalia com score 0-100 e crÃ­ticas devastadoras
  3. **LoRA Training**: Treinamento incremental real com Unsloth (se habilitado)
  4. **HERMES**: Refina draft baseado em crÃ­ticas
  5. Repete atÃ© `target_quality` ou `max_iters`

### `BeagleLoRATraining` (LoRA Training Real)
- `full_training_pipeline(bad_drafts, good_drafts, contexts; ...)` - Pipeline completo:
  1. **create_training_dataset()**: Formata pares (badâ†’good) para Llama-3.3
  2. **load_model_and_tokenizer()**: Carrega Llama-3.3-70B com 4bit + LoRA adapters
  3. **train_lora()**: Treina adapter com Unsloth (5-10 min no cluster)
  4. **save_adapter_gguf()**: Salva adapter GGUF pronto para vLLM
- `collect_adversarial_pairs(history)`: Coleta pares do histÃ³rico adversarial

## ğŸ”¥ PrÃ³ximos Passos

### Fase 1: Core Quantum (âœ… COMPLETO)
- [x] Superposition
- [x] Interference
- [x] Measurement/Collapse
- [x] Entropy calculation

### Fase 2: Embeddings & Semantic Interference
- [ ] IntegraÃ§Ã£o com TextEmbeddings.jl ou API HTTP
- [ ] InterferÃªncia baseada em similaridade semÃ¢ntica real
- [ ] Cosine similarity para matching de evidÃªncias

### Fase 3: Multi-Agent Orchestrator (âœ… COMPLETO)
- [x] Portar `beagle-hermes` para Julia
- [x] IntegraÃ§Ã£o ATHENA + HERMES + ARGOS
- [x] Adversarial self-play loop completo
- [x] IntegraÃ§Ã£o orchestrator â†’ adversarial automÃ¡tico

### Fase 4: LoRA Training (âœ… COMPLETO)
- [x] LoRA training real com Unsloth via PythonCall
- [x] Treinamento incremental apÃ³s cada adversarial iteration
- [x] Adapter GGUF salvo para uso imediato no vLLM
- [x] IntegraÃ§Ã£o automÃ¡tica adversarial â†’ LoRA

### Fase 5: Fractal Core
- [ ] FractalCognitiveNode recursivo
- [ ] Holographic compression real (embeddings)
- [ ] Auto-replicaÃ§Ã£o com controle de recursos

## ğŸ§ª Testes

```julia
using Test
using BeagleQuantum

@testset "BeagleQuantum" begin
    set = HypothesisSet()
    add!(set, "Test hypothesis")
    @test length(set.hyps) == 1
    @test set.hyps[1].probability > 0.0
end
```

## ğŸ“Š Performance

Benchmarks comparativos (em breve):
- vs Python (numpy): ~50-100x mais rÃ¡pido
- vs Rust: ~2-5x mais rÃ¡pido (devido a otimizaÃ§Ãµes de Julia)
- vs C++: comparÃ¡vel, mas com sintaxe muito mais limpa

## ğŸ”— IntegraÃ§Ã£o com BEAGLE Rust

O cluster Rust continua servindo:
- **vLLM** para geraÃ§Ã£o de hipÃ³teses
- **Neo4j** para knowledge graph
- **API HTTP** para comunicaÃ§Ã£o Julia â†” Rust

Julia foca em:
- **RaciocÃ­nio quÃ¢ntico** (superposition, interference)
- **LoRA training** (Lux + MLX no M3 Max)
- **SimulaÃ§Ãµes numÃ©ricas** (fractais, entropia)

## ğŸ“ LicenÃ§a

Mesma licenÃ§a do BEAGLE principal.

## ğŸ¤ Contribuindo

Este Ã© um projeto de pesquisa interdisciplinar. ContribuiÃ§Ãµes sÃ£o bem-vindas, especialmente em:
- OtimizaÃ§Ãµes de performance
- IntegraÃ§Ã£o com ecosistema ML Julia
- Testes e benchmarks

---

**2026 serÃ¡ o ano do BEAGLE em Julia.** ğŸ”¥


